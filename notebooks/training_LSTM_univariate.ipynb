{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319add11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58a15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/syntychefabien/Documents/Syntyche/Isheero/BootCamp_ATUT2023/PowerForecast_temp/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if \"notebook\" in current_dir:\n",
    "    current_dir = current_dir.split(\"notebook\")[0]\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "# Change the current working directory\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e2f27",
   "metadata": {},
   "source": [
    "## Chargement les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06561704",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_gathering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Charger les données à partir du fichier csv\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/PowerConsumptionTetouan/Tetuan_City_power_consumption.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_gathering\u001b[49m\u001b[38;5;241m.\u001b[39mload_dataset(data_filename, col_sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, b_rename_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_gathering' is not defined"
     ]
    }
   ],
   "source": [
    "from src import data_gathering, data_analysis, data_featuring\n",
    "\n",
    "# Charger les données à partir du fichier csv\n",
    "data_filename = \"data/PowerConsumptionTetouan/Tetuan_City_power_consumption.csv\"\n",
    "df_dataset = data_gathering.load_dataset(data_filename, col_sep=\",\", b_rename_cols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73446777",
   "metadata": {},
   "source": [
    "## Définition des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3160a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres des entrées et sortie\n",
    "feat_name = \"Consumption_Z3\"\n",
    "\n",
    "# Paramètres pour la création des bases de test et d'apprentissage\n",
    "train_ratio = 0.8\n",
    "win_size_hours = 1\n",
    "time_sampling_mins = 10\n",
    "b_scaler = True\n",
    "scaler_str = \"std\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556abf92",
   "metadata": {},
   "source": [
    "## Création des bases d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a295c",
   "metadata": {},
   "source": [
    "#### Séparation temporelles des bases d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020fc0b",
   "metadata": {},
   "source": [
    "#### Création des séquences temporelles \n",
    "Il faudra structurer les données en séquences temporelles afin de prendre en compte un historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bb235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, idx_train, X_test, y_test, idx_test, scaler = \\\n",
    "training.build_train_test_univariate_sequences(df_data=df_dataset,\n",
    "                                               feat_name=feat_name,\n",
    "                                               win_size_hours=win_size_hours,\n",
    "                                               time_sampling_mins=time_sampling_mins,\n",
    "                                               train_ratio=train_ratio,\n",
    "                                               scaler_str=scaler_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab045f49",
   "metadata": {},
   "source": [
    "## Prédiction de la consommation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baec99",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b559bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Paramètres d'apprentissage\n",
    "d_learning_params = dict()\n",
    "d_learning_params['batch_size'] = 256\n",
    "d_learning_params['epochs'] = 50\n",
    "d_learning_params['activation_fn'] = \"relu\"\n",
    "d_learning_params['loss_fn'] = \"mse\"\n",
    "d_learning_params['optimizer'] = \"adam\"\n",
    "d_learning_params['val_ratio'] = 0.2\n",
    "d_learning_params['win_size_hours'] = win_size_hours\n",
    "d_learning_params['time_sampling_mins'] = time_sampling_mins\n",
    "d_learning_params['scaler'] = scaler\n",
    "d_learning_params['feat_names'] = feat_name\n",
    "d_learning_params['target_names'] = feat_name\n",
    "d_learning_params['metrics'] = ['mae']\n",
    "d_learning_params['d_train_test'] = {}\n",
    "d_learning_params['seq_train_test'] = {\"train\":{\"features\":X_train, \"targets\":y_train, \"targets_idx\": idx_train},\n",
    "                                       \"test\":{\"features\":X_test, \"targets\":y_test, \"targets_idx\": idx_test}}\n",
    "d_learning_params['layers'] = [100, 50, 10]\n",
    "\n",
    "# Récupérer la date et l'heure pour la création du fichier du modèle\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = os.path.join(\"models\", \"Univariate_LSTM_{}h\".format(win_size_hours), feat_name, current_datetime)\n",
    "params_file = os.path.join(results_dir, \"lstm_params_{}.pkl\".format(str(current_datetime)))\n",
    "\n",
    "# Création du répertoire de sauvegarde des résultats s'il n'existe pas déjà\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Sauvergarde des paramètres d'appentissag\n",
    "with open(params_file, 'wb') as fp:\n",
    "    pickle.dump(d_learning_params, fp)\n",
    "\n",
    "model_lstm, scores = training.train_lstm_univariate(X_train, y_train,\n",
    "                                         X_test, y_test, \n",
    "                                         d_learning_params,\n",
    "                                         results_dir)\n",
    "\n",
    "# Sauvegarder les scores de ce modèle\n",
    "d_scores = d_learning_params.copy()\n",
    "d_scores[\"scores\"] = scores\n",
    "scores_file = os.path.join(results_dir, \"lstm_scores_{}.pkl\".format(str(current_datetime)))\n",
    "\n",
    "print(\"Performance du modèle\")\n",
    "print(\"\\t --> Score R2 (Le modèle parfait a un score de 1)\")\n",
    "print(\"\\t \\t --> Ensemble d'apprentissage : {}\".format(scores['R2']['train']))\n",
    "print(\"\\t \\t --> Ensemble de test : {}\".format(scores['R2']['test']))\n",
    "print(\"\\n\")\n",
    "print(\"\\t --> MAE (Erreur moyenne absolue) en KW\")\n",
    "print(\"\\t \\t --> Ensemble d'apprentissage : {}\".format(scores['MAE']['train']))\n",
    "print(\"\\t \\t --> Ensemble de test : {}\".format(scores['MAE']['test']))\n",
    "\n",
    "# Sauvergarde des paramètres d'appentissag\n",
    "with open(scores_file, 'wb') as fp:\n",
    "    pickle.dump(d_scores, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f8d1e",
   "metadata": {},
   "source": [
    "### Affichage des prédictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa83f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.prediction import display_prediction\n",
    "\n",
    "# Calculer les prédictions\n",
    "# Attention à appliquer la normalisation inverse pour retrouver l'échelle initiale\n",
    "pred_col_name = [\"prediction\"]\n",
    "df_test = pd.DataFrame(data=d_learning_params['scaler'].inverse_transform(y_test), columns=[feat_name],index=idx_test)\n",
    "df_test[pred_col_name[0]] = d_learning_params['scaler'].inverse_transform(model_lstm.predict(X_test))\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "df_test.to_pickle(os.path.join(results_dir, \"lstm_predictions_{}_{}.pkl\".format(feat_name, str(current_datetime))))\n",
    "\n",
    "# Afficher les résultats\n",
    "display_prediction(df_test, target_col_name=feat_name,\n",
    "                   pred_col_name=pred_col_name, figsize=(14, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71db5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"2017-11-20\":\"2017-11-30\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33106ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
